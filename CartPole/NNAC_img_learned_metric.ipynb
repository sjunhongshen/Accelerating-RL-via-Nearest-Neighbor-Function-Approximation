{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of image cart pole work",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMc7LU19U1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "a3a623da-f6db-4cdc-8161-5b21607aad41"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!apt-get update\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [898 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 1,154 kB in 1s (842 kB/s)\n",
            "Reading package lists... Done\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.6).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 76 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlP0wX79gXH",
        "colab_type": "text"
      },
      "source": [
        "Import essentials and set up display.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9Z6uOOy9pL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym, copy, math, cv2\n",
        "from gym.wrappers import Monitor\n",
        "from skimage.color import rgb2gray\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob, io, base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbv2Hq2q905W",
        "colab_type": "text"
      },
      "source": [
        "Load distance model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aABYrmw790EU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "729b6375-22c8-43a5-a5c5-fafa7162248a"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "filepath = \"/content/drive/My Drive/distance.h5\"\n",
        "model = load_model(filepath)\n",
        "\n",
        "def predict(a, b):\n",
        "    a = np.transpose(a, (2, 0, 1))\n",
        "    b = np.transpose(b, (2, 0, 1))\n",
        "    data = np.reshape(np.concatenate((a, b), axis = 0), (-1, 4, ROWS, COLS))\n",
        "    \n",
        "    pre = model.predict(data)\n",
        "    return pre[0][0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02pnzDGs94Zf",
        "colab_type": "text"
      },
      "source": [
        "Parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAIi_B5294zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_EPISODE = 5000\n",
        "MAX_EP_STEPS = 500 \n",
        "RENDER = False\n",
        "LR_A = 0.0005 \n",
        "ROWS = 160\n",
        "COLS = 240\n",
        "state_size = (1, ROWS, COLS, 4)\n",
        "COLS2 = 20\n",
        "ROWS2 = 20"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imsaCGHm-BsX",
        "colab_type": "text"
      },
      "source": [
        "Helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikIRQflCBd5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_img(img):\n",
        "    shape = (COLS2, ROWS2)\n",
        "    pixels = img.flatten()[::-1] \n",
        "    ylen = img.shape[1]\n",
        "    flag = False\n",
        "    idx1 = idx2 = None\n",
        "    for idx in range(len(pixels)):\n",
        "        if pixels[idx] == 1:\n",
        "            if not flag:\n",
        "                idx1 = idx\n",
        "            flag = True\n",
        "        if flag and pixels[idx] == 0:\n",
        "            idx2 = idx\n",
        "            break\n",
        "    middle = int((idx2 + idx1) / 2)\n",
        "    x = int((len(pixels) - middle) / img.shape[1])\n",
        "    y = (len(pixels) - middle) % img.shape[1]\n",
        "    starty = max(0, y - 40)\n",
        "    startx = max(0, x - 145)\n",
        "    img = np.array(img[startx:startx+80, starty:starty+80], dtype=np.float32)\n",
        "    img = np.array(np.array(cv2.resize(img, (shape[0], shape[1])), dtype = bool), dtype=int)\n",
        "    return np.reshape(img, (ROWS2, COLS2, 1))\n",
        "\n",
        "def adaptive_lr(solve, lr_episode, t, flag):\n",
        "    if t >= 350 and not flag:\n",
        "        flag = True\n",
        "    if solve == 2: return max(501, lr_episode), flag\n",
        "    if solve == 3: return max(701, lr_episode), flag\n",
        "    if solve == 5: return max(901, lr_episode), flag\n",
        "    return lr_episode, flag"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGd-5MFWBc7x",
        "colab_type": "text"
      },
      "source": [
        "Actor class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgpJS-vtBqSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(object):\n",
        "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
        "        self.sess = sess\n",
        "        self.s = tf.placeholder(tf.float32, [1, ROWS2, COLS2, 4], \"state\")\n",
        "        self.a = tf.placeholder(tf.int32, (), \"act\")\n",
        "        self.td_error = tf.placeholder(tf.float32, (), \"td_error\") \n",
        "        self.TAU = 0.6\n",
        "        self.p = 1\n",
        "        self.changeflag = True\n",
        "\n",
        "        with tf.variable_scope('Actor'):\n",
        "            filter = tf.Variable(tf.random_normal([5, 5, 1, 16], mean = 0.0, stddev = 0.01))\n",
        "            l1 = tf.nn.conv2d(self.s, filter, strides = 2, padding = \"SAME\")\n",
        "            l1 = tf.layers.flatten(l1)\n",
        "\n",
        "            self.l1 = tf.layers.dense(inputs = l1, units = 20, activation = None, kernel_initializer = tf.random_normal_initializer(0., .01),\n",
        "                bias_initializer = tf.constant_initializer(0.01),  name = 'l1')\n",
        "\n",
        "            self.acts_prob = tf.layers.dense(inputs = self.l1, units = n_actions, activation = tf.nn.softmax, kernel_initializer = tf.random_normal_initializer(0., .01),  # weights\n",
        "                bias_initializer = tf.constant_initializer(0.01), name = 'acts_prob')\n",
        "\n",
        "        with tf.variable_scope('logprob'):\n",
        "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
        "            self.exp_v = tf.reduce_mean(log_prob) \n",
        "            \n",
        "        with tf.variable_scope('train'):\n",
        "            self.global_step = tf.Variable(0, trainable = False)\n",
        "            self.learning_rate = tf.train.exponential_decay(LR_A, self.global_step, 100, 0.5, staircase = True)\n",
        "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "            self.gvs = self.optimizer.compute_gradients(-self.exp_v)\n",
        "\n",
        "            def multtd(grad):\n",
        "                if grad is None:\n",
        "                    return grad\n",
        "                return self.td_error * grad\n",
        "\n",
        "            self.new_gvs = [(multtd(grad), var) for grad, var in self.gvs]\n",
        "            self.train_op = self.optimizer.apply_gradients(self.new_gvs)\n",
        "\n",
        "        with tf.variable_scope('Target'):\n",
        "            tfilter = tf.Variable(tf.random_normal([5, 5, 1, 16], mean = 0.0, stddev = 0.01))\n",
        "            tl1 = tf.nn.conv2d(self.s, tfilter, strides = 2, padding = \"SAME\")\n",
        "            tl1 = tf.layers.flatten(tl1)\n",
        "            self.tl1 = tf.layers.dense(inputs = l1, units = 20, activation = None, kernel_initializer = tf.random_normal_initializer(0., .01),\n",
        "                bias_initializer = tf.constant_initializer(0.01), name = 'l1')\n",
        "            self.tacts_prob = tf.layers.dense(inputs = self.l1, units = n_actions, activation = tf.nn.softmax, kernel_initializer = tf.random_normal_initializer(0., .01),  # weights\n",
        "                bias_initializer = tf.constant_initializer(0.01), name = 'acts_prob')\n",
        "\n",
        "    def learn(self, s, a, td, run):\n",
        "        s = np.reshape(s, (1, ROWS2, COLS2, 4))\n",
        "        feed_dict = {self.s: s, self.a: a, self.td_error: td, self.global_step:run}\n",
        "        _, exp_v, ap, l1= self.sess.run([self.train_op, self.exp_v, self.acts_prob, self.l1], feed_dict)\n",
        "       \n",
        "        if run == 120 and self.changeflag:\n",
        "            self.p = 0\n",
        "            self.changeflag = False\n",
        "        return exp_v\n",
        "\n",
        "    def choose_action(self, s, hardmax=False):\n",
        "        s = np.reshape(s, (1, ROWS2, COLS2, 4))\n",
        "        probs = self.sess.run([self.tacts_prob], {self.s: s})[0]\n",
        "        \n",
        "        if hardmax:\n",
        "            return np.argmax(probs[0])\n",
        "        return np.random.choice(np.arange(probs.shape[1]), p = probs.ravel())\n",
        "\n",
        "    def init_target(self):\n",
        "        actor_var_names = tf.trainable_variables(\"Actor\")\n",
        "        actor_var = self.sess.run(actor_var_names)\n",
        "        target_var_names = tf.trainable_variables(\"Target\")\n",
        "        for i, j in zip(target_var_names, actor_var):\n",
        "            self.sess.run(i.assign(j))\n",
        "\n",
        "    def update_var(self):\n",
        "        actor_var_names = tf.trainable_variables(\"Actor\")\n",
        "        actor_var = self.sess.run(actor_var_names)\n",
        "        target_var_names = tf.trainable_variables(\"Target\")\n",
        "        target_var = self.sess.run(target_var_names)\n",
        "        for a, i, j, k in zip(actor_var_names, target_var_names, target_var, actor_var):\n",
        "            self.sess.run(i.assign(self.TAU * j + (1 - self.TAU) * k))\n",
        "            if self.p != 0:\n",
        "                self.sess.run(a.assign(k + self.p * np.random.normal(0, 5e-3 ,size = k.shape)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdU8djP-Bpkh",
        "colab_type": "text"
      },
      "source": [
        "Critic class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4NBTeOQB2gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NNCritic:\n",
        "    def __init__(self, observation_space, action_space, actor):\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        \n",
        "        # parameters to tune\n",
        "        self.H = 12\n",
        "        self.L = 0.5\n",
        "        self.discount = 0.99\n",
        "\n",
        "        self.max_size = 500000\n",
        "        self.ptr = int(0)\n",
        "        self.next_states, self.next_states2, self.next_states_full = [], [], []\n",
        "        self.states_all, self.states_full = [], []\n",
        "        self.rewards = []\n",
        "\n",
        "        self.tree = None\n",
        "        self.actor = actor\n",
        "        self.weights =  np.array([1 for i in range(self.observation_space)] + [1])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, step, terminal, img, img_next, s_f, s_f_):\n",
        "        if len(self.states_all) == self.max_size:\n",
        "            self.states_all[self.ptr] = np.concatenate((state, [action]))\n",
        "            self.states_full[self.ptr] = s_f\n",
        "            self.next_states[self.ptr] = next_state\n",
        "            self.next_states2[self.ptr] = img_next.flatten()\n",
        "            self.next_states_full[self.ptr] = s_f_\n",
        "            self.rewards[self.ptr] = reward\n",
        "            self.ptr = int((self.ptr + 1) % self.max_size)\n",
        "        else:\n",
        "            self.states_all.append(np.concatenate((state, [action])))\n",
        "            self.states_full.append(s_f)\n",
        "            self.next_states.append(next_state)\n",
        "            self.next_states2.append(img_next.flatten())\n",
        "            self.next_states_full.append(s_f_)\n",
        "            self.rewards.append(reward)\n",
        "            \n",
        "    def act(self, step, state, img, sf):\n",
        "        if step == self.H:\n",
        "            return state, 0\n",
        "\n",
        "        img_ = np.concatenate((np.reshape(img.flatten()[:ROWS2 * COLS2], (ROWS2, COLS2, 1)), np.reshape(img.flatten()[ROWS2 * COLS2:2 * ROWS2 * COLS2], (ROWS2, COLS2, 1)), np.reshape(img.flatten()[2 * ROWS2 * COLS2:3 * ROWS2 * COLS2], (ROWS2, COLS2, 1)), np.reshape(img.flatten()[3 * ROWS2 * COLS2:], (ROWS2, COLS2, 1))), axis = 2)\n",
        "        a = self.actor.choose_action(img_)\n",
        "\n",
        "        dd, ii = self.tree.query(self.weights * np.concatenate((state, [a])), k = 1, n_jobs = -1)\n",
        "        nearest = self.states_all[ii]\n",
        "  \n",
        "        dd = predict(self.states_full[ii], sf) if dd !=0 else 0\n",
        "        \n",
        "        if self.rewards[ii] == -1:\n",
        "            return nearest[:-1], -1 + self.L * dd\n",
        "        else:\n",
        "            q = self.rewards[ii] + self.discount * self.act(step + 1, self.next_states[ii], self.next_states2[ii], self.next_states_full[ii])[1] + self.L * dd\n",
        "            return nearest[:-1], q\n",
        "        \n",
        "    def learn(self, s, r, s_, run, step, img, img_next, sf, sf_, flag):\n",
        "        if run == 0: return 0\n",
        "        if not flag:\n",
        "            self.tree = cKDTree(self.weights * self.states_all)\n",
        "        \n",
        "        n, v = self.act(0, s, img.flatten(), sf)\n",
        "        n_, v_ = self.act(1, s_, img_next.flatten(), sf_)\n",
        "        td_error = r + self.discount * v_ - v\n",
        "\n",
        "        return td_error"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDU1p5xgB291",
        "colab_type": "text"
      },
      "source": [
        "Run experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q52SrQxHJ_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee3ef994-b5ea-430a-ec39-e30b763979cf"
      },
      "source": [
        "seed = 1\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "env = wrap_env(gym.make('CartPole-v1'))\n",
        "env = env.unwrapped\n",
        "env.seed(seed)\n",
        "env._max_episode_steps = 500\n",
        "N_F = env.observation_space.shape[0]\n",
        "N_A = env.action_space.n\n",
        "\n",
        "sess = tf.Session()\n",
        "actor = Actor(sess, n_features = 4 * ROWS2 * COLS2, n_actions = N_A, lr = LR_A)\n",
        "critic = NNCritic(N_F, N_A, actor)   \n",
        "sess.run(tf.global_variables_initializer())\n",
        "actor.init_target()\n",
        "\n",
        "scores = []\n",
        "flag = False\n",
        "timesteps, running_reward = 0, 0\n",
        "i_episode, lr_episode = 0, 0\n",
        "solve = 0\n",
        "while i_episode < MAX_EPISODE:\n",
        "    timesteps += 1\n",
        "    s = env.reset()\n",
        "    t = 0\n",
        "    track_r = []\n",
        "\n",
        "    img_ = np.array(np.array((env.render(mode = 'rgb_array')[:,:,0]) - 255, dtype = bool), dtype=np.float32)\n",
        "    img_prev1 = img_prev2 = img_prev = img = crop_img(img_)\n",
        "    img_prev_full = img_full = np.reshape(cv2.resize(img_, (COLS, ROWS)), (ROWS, COLS, 1))\n",
        "\n",
        "    while True:\n",
        "        if RENDER: env.render()\n",
        "        \n",
        "        a = actor.choose_action(np.concatenate((img_prev1, img_prev2, img_prev, img), axis = 2))\n",
        "        s_, r, done, info = env.step(a)\n",
        "        if done: r = -1\n",
        "\n",
        "        img_next_ = np.array(np.array((env.render(mode = 'rgb_array')[:,:,0]) - 255, dtype = bool), dtype = np.float32)\n",
        "        img_next = crop_img(img_next_)\n",
        "        img_next_full = np.reshape(cv2.resize(img_next_, (COLS, ROWS)), (ROWS, COLS, 1))\n",
        "     \n",
        "        critic.remember(s, a, r, s_, t, done, np.concatenate((img_prev1, img_prev2, img_prev, img)), np.concatenate((img_prev2, img_prev,img, img_next)), np.concatenate((img_prev_full, img_full), axis = 2), np.concatenate((img_full, img_next_full), axis = 2))\n",
        "        track_r.append(r)\n",
        "\n",
        "        td_error = np.clip(critic.learn(s, r, s_, i_episode, t, np.concatenate((img_prev1, img_prev2, img_prev, img)), np.concatenate((img_prev2, img_prev,img, img_next)), np.concatenate((img_prev_full, img_full), axis = 2), np.concatenate((img_full, img_next_full), axis = 2), flag), -3, 3)\n",
        "\n",
        "        actor.learn(np.concatenate((img_prev1, img_prev2, img_prev, img), axis = 2), a, td_error, lr_episode) \n",
        "\n",
        "        img_prev1 = img_prev2\n",
        "        img_prev2 = img_prev\n",
        "        img_prev = img\n",
        "        img = img_next\n",
        "\n",
        "        img_prev_full = img_full\n",
        "        img_full = img_next_full\n",
        "        s = s_\n",
        "\n",
        "        t += 1\n",
        "        timesteps += 1\n",
        "\n",
        "        if done or t >= MAX_EP_STEPS:\n",
        "            if t == MAX_EP_STEPS: solve += 1\n",
        "            lr_episode, flag = adaptive_lr(solve, lr_episode, t, flag)\n",
        "            \n",
        "            if flag:\n",
        "                critic.tree = cKDTree(critic.weights * critic.states_all)\n",
        "\n",
        "            if i_episode % 2 == 0:\n",
        "                actor.update_var()\n",
        "            \n",
        "            ep_rs_sum = sum(track_r)\n",
        "            running_reward = running_reward * 0.8 + ep_rs_sum * 0.2\n",
        "            print(\"Episode: \", i_episode, \" running reward: \", running_reward, \" episode reward: \", ep_rs_sum)\n",
        "            scores.append(ep_rs_sum)\n",
        "            i_episode += 1\n",
        "            lr_episode += 1\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-f3187d4615a2>:14: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-f3187d4615a2>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "Episode:  0  running reward:  2.2  episode reward:  11.0\n",
            "Episode:  1  running reward:  5.16  episode reward:  17.0\n",
            "Episode:  2  running reward:  5.928  episode reward:  9.0\n",
            "Episode:  3  running reward:  9.1424  episode reward:  22.0\n",
            "Episode:  4  running reward:  9.31392  episode reward:  10.0\n",
            "Episode:  5  running reward:  10.051136  episode reward:  13.0\n",
            "Episode:  6  running reward:  11.4409088  episode reward:  17.0\n",
            "Episode:  7  running reward:  12.952727040000001  episode reward:  19.0\n",
            "Episode:  8  running reward:  13.162181632000003  episode reward:  14.0\n",
            "Episode:  9  running reward:  13.129745305600002  episode reward:  13.0\n",
            "Episode:  10  running reward:  15.503796244480002  episode reward:  25.0\n",
            "Episode:  11  running reward:  16.403036995584003  episode reward:  20.0\n",
            "Episode:  12  running reward:  19.922429596467204  episode reward:  34.0\n",
            "Episode:  13  running reward:  20.33794367717376  episode reward:  22.0\n",
            "Episode:  14  running reward:  19.670354941739014  episode reward:  17.0\n",
            "Episode:  15  running reward:  18.136283953391214  episode reward:  12.0\n",
            "Episode:  16  running reward:  17.309027162712972  episode reward:  14.0\n",
            "Episode:  17  running reward:  16.24722173017038  episode reward:  12.0\n",
            "Episode:  18  running reward:  14.797777384136307  episode reward:  9.0\n",
            "Episode:  19  running reward:  14.838221907309046  episode reward:  15.0\n",
            "Episode:  20  running reward:  14.070577525847238  episode reward:  11.0\n",
            "Episode:  21  running reward:  17.656462020677793  episode reward:  32.0\n",
            "Episode:  22  running reward:  16.125169616542237  episode reward:  10.0\n",
            "Episode:  23  running reward:  17.70013569323379  episode reward:  24.0\n",
            "Episode:  24  running reward:  21.160108554587033  episode reward:  35.0\n",
            "Episode:  25  running reward:  20.52808684366963  episode reward:  18.0\n",
            "Episode:  26  running reward:  23.022469474935708  episode reward:  33.0\n",
            "Episode:  27  running reward:  20.01797557994857  episode reward:  8.0\n",
            "Episode:  28  running reward:  24.014380463958855  episode reward:  40.0\n",
            "Episode:  29  running reward:  22.411504371167084  episode reward:  16.0\n",
            "Episode:  30  running reward:  20.329203496933665  episode reward:  12.0\n",
            "Episode:  31  running reward:  18.263362797546932  episode reward:  10.0\n",
            "Episode:  32  running reward:  30.21069023803755  episode reward:  78.0\n",
            "Episode:  33  running reward:  27.768552190430043  episode reward:  18.0\n",
            "Episode:  34  running reward:  31.214841752344036  episode reward:  45.0\n",
            "Episode:  35  running reward:  30.371873401875227  episode reward:  27.0\n",
            "Episode:  36  running reward:  33.29749872150018  episode reward:  45.0\n",
            "Episode:  37  running reward:  30.43799897720015  episode reward:  19.0\n",
            "Episode:  38  running reward:  28.750399181760123  episode reward:  22.0\n",
            "Episode:  39  running reward:  27.600319345408103  episode reward:  23.0\n",
            "Episode:  40  running reward:  36.28025547632649  episode reward:  71.0\n",
            "Episode:  41  running reward:  34.22420438106119  episode reward:  26.0\n",
            "Episode:  42  running reward:  29.979363504848955  episode reward:  13.0\n",
            "Episode:  43  running reward:  30.383490803879162  episode reward:  32.0\n",
            "Episode:  44  running reward:  31.906792643103334  episode reward:  38.0\n",
            "Episode:  45  running reward:  32.125434114482665  episode reward:  33.0\n",
            "Episode:  46  running reward:  32.700347291586134  episode reward:  35.0\n",
            "Episode:  47  running reward:  29.160277833268907  episode reward:  15.0\n",
            "Episode:  48  running reward:  29.32822226661513  episode reward:  30.0\n",
            "Episode:  49  running reward:  34.6625778132921  episode reward:  56.0\n",
            "Episode:  50  running reward:  32.93006225063368  episode reward:  26.0\n",
            "Episode:  51  running reward:  34.74404980050695  episode reward:  42.0\n",
            "Episode:  52  running reward:  29.395239840405562  episode reward:  8.0\n",
            "Episode:  53  running reward:  25.116191872324453  episode reward:  8.0\n",
            "Episode:  54  running reward:  21.692953497859566  episode reward:  8.0\n",
            "Episode:  55  running reward:  19.554362798287652  episode reward:  11.0\n",
            "Episode:  56  running reward:  22.243490238630123  episode reward:  33.0\n",
            "Episode:  57  running reward:  29.7947921909041  episode reward:  60.0\n",
            "Episode:  58  running reward:  25.635833752723283  episode reward:  9.0\n",
            "Episode:  59  running reward:  28.908667002178625  episode reward:  42.0\n",
            "Episode:  60  running reward:  28.3269336017429  episode reward:  26.0\n",
            "Episode:  61  running reward:  29.461546881394323  episode reward:  34.0\n",
            "Episode:  62  running reward:  33.76923750511546  episode reward:  51.0\n",
            "Episode:  63  running reward:  34.81539000409237  episode reward:  39.0\n",
            "Episode:  64  running reward:  37.6523120032739  episode reward:  49.0\n",
            "Episode:  65  running reward:  32.12184960261912  episode reward:  10.0\n",
            "Episode:  66  running reward:  37.8974796820953  episode reward:  61.0\n",
            "Episode:  67  running reward:  32.317983745676244  episode reward:  10.0\n",
            "Episode:  68  running reward:  31.054386996540995  episode reward:  26.0\n",
            "Episode:  69  running reward:  27.443509597232797  episode reward:  13.0\n",
            "Episode:  70  running reward:  26.95480767778624  episode reward:  25.0\n",
            "Episode:  71  running reward:  27.363846142228994  episode reward:  29.0\n",
            "Episode:  72  running reward:  27.891076913783195  episode reward:  30.0\n",
            "Episode:  73  running reward:  29.312861531026556  episode reward:  35.0\n",
            "Episode:  74  running reward:  24.850289224821246  episode reward:  7.0\n",
            "Episode:  75  running reward:  28.480231379856995  episode reward:  43.0\n",
            "Episode:  76  running reward:  29.984185103885597  episode reward:  36.0\n",
            "Episode:  77  running reward:  30.98734808310848  episode reward:  35.0\n",
            "Episode:  78  running reward:  33.389878466486785  episode reward:  43.0\n",
            "Episode:  79  running reward:  34.91190277318943  episode reward:  41.0\n",
            "Episode:  80  running reward:  39.52952221855155  episode reward:  58.0\n",
            "Episode:  81  running reward:  45.02361777484124  episode reward:  67.0\n",
            "Episode:  82  running reward:  39.618894219873  episode reward:  18.0\n",
            "Episode:  83  running reward:  40.695115375898396  episode reward:  45.0\n",
            "Episode:  84  running reward:  39.15609230071872  episode reward:  33.0\n",
            "Episode:  85  running reward:  36.524873840574976  episode reward:  26.0\n",
            "Episode:  86  running reward:  37.81989907245998  episode reward:  43.0\n",
            "Episode:  87  running reward:  38.25591925796799  episode reward:  40.0\n",
            "Episode:  88  running reward:  37.80473540637439  episode reward:  36.0\n",
            "Episode:  89  running reward:  39.243788325099516  episode reward:  45.0\n",
            "Episode:  90  running reward:  37.79503066007961  episode reward:  32.0\n",
            "Episode:  91  running reward:  43.63602452806369  episode reward:  67.0\n",
            "Episode:  92  running reward:  42.308819622450955  episode reward:  37.0\n",
            "Episode:  93  running reward:  43.047055697960765  episode reward:  46.0\n",
            "Episode:  94  running reward:  39.03764455836861  episode reward:  23.0\n",
            "Episode:  95  running reward:  36.23011564669489  episode reward:  25.0\n",
            "Episode:  96  running reward:  47.58409251735591  episode reward:  93.0\n",
            "Episode:  97  running reward:  48.067274013884735  episode reward:  50.0\n",
            "Episode:  98  running reward:  44.653819211107795  episode reward:  31.0\n",
            "Episode:  99  running reward:  40.92305536888624  episode reward:  26.0\n",
            "Episode:  100  running reward:  36.73844429510899  episode reward:  20.0\n",
            "Episode:  101  running reward:  33.99075543608719  episode reward:  23.0\n",
            "Episode:  102  running reward:  33.592604348869756  episode reward:  32.0\n",
            "Episode:  103  running reward:  34.8740834790958  episode reward:  40.0\n",
            "Episode:  104  running reward:  37.499266783276646  episode reward:  48.0\n",
            "Episode:  105  running reward:  58.19941342662132  episode reward:  141.0\n",
            "Episode:  106  running reward:  54.359530741297064  episode reward:  39.0\n",
            "Episode:  107  running reward:  51.687624593037654  episode reward:  41.0\n",
            "Episode:  108  running reward:  49.55009967443013  episode reward:  41.0\n",
            "Episode:  109  running reward:  51.640079739544106  episode reward:  60.0\n",
            "Episode:  110  running reward:  52.71206379163529  episode reward:  57.0\n",
            "Episode:  111  running reward:  51.36965103330824  episode reward:  46.0\n",
            "Episode:  112  running reward:  84.09572082664658  episode reward:  215.0\n",
            "Episode:  113  running reward:  91.47657666131727  episode reward:  121.0\n",
            "Episode:  114  running reward:  108.98126132905381  episode reward:  179.0\n",
            "Episode:  115  running reward:  88.78500906324305  episode reward:  8.0\n",
            "Episode:  116  running reward:  103.62800725059444  episode reward:  163.0\n",
            "Episode:  117  running reward:  100.70240580047555  episode reward:  89.0\n",
            "Episode:  118  running reward:  85.36192464038044  episode reward:  24.0\n",
            "Episode:  119  running reward:  96.28953971230435  episode reward:  140.0\n",
            "Episode:  120  running reward:  131.83163176984348  episode reward:  274.0\n",
            "Episode:  121  running reward:  111.6653054158748  episode reward:  31.0\n",
            "Episode:  122  running reward:  103.13224433269984  episode reward:  69.0\n",
            "Episode:  123  running reward:  101.50579546615988  episode reward:  95.0\n",
            "Episode:  124  running reward:  91.20463637292791  episode reward:  50.0\n",
            "Episode:  125  running reward:  86.76370909834233  episode reward:  69.0\n",
            "Episode:  126  running reward:  84.21096727867386  episode reward:  74.0\n",
            "Episode:  127  running reward:  81.96877382293908  episode reward:  73.0\n",
            "Episode:  128  running reward:  107.17501905835127  episode reward:  208.0\n",
            "Episode:  129  running reward:  114.54001524668102  episode reward:  144.0\n",
            "Episode:  130  running reward:  127.43201219734482  episode reward:  179.0\n",
            "Episode:  131  running reward:  168.74560975787585  episode reward:  334.0\n",
            "Episode:  132  running reward:  187.1964878063007  episode reward:  261.0\n",
            "Episode:  133  running reward:  198.15719024504057  episode reward:  242.0\n",
            "Episode:  134  running reward:  160.12575219603247  episode reward:  8.0\n",
            "Episode:  135  running reward:  146.10060175682597  episode reward:  90.0\n",
            "Episode:  136  running reward:  141.08048140546077  episode reward:  121.0\n",
            "Episode:  137  running reward:  118.26438512436863  episode reward:  27.0\n",
            "Episode:  138  running reward:  137.61150809949493  episode reward:  215.0\n",
            "Episode:  139  running reward:  125.48920647959595  episode reward:  77.0\n",
            "Episode:  140  running reward:  200.39136518367678  episode reward:  500.0\n",
            "Episode:  141  running reward:  201.51309214694146  episode reward:  206.0\n",
            "Episode:  142  running reward:  230.81047371755318  episode reward:  348.0\n",
            "Episode:  143  running reward:  216.64837897404254  episode reward:  160.0\n",
            "Episode:  144  running reward:  207.91870317923403  episode reward:  173.0\n",
            "Episode:  145  running reward:  207.73496254338724  episode reward:  207.0\n",
            "Episode:  146  running reward:  201.1879700347098  episode reward:  175.0\n",
            "Episode:  147  running reward:  162.75037602776786  episode reward:  9.0\n",
            "Episode:  148  running reward:  143.8003008222143  episode reward:  68.0\n",
            "Episode:  149  running reward:  116.64024065777144  episode reward:  8.0\n",
            "Episode:  150  running reward:  94.91219252621715  episode reward:  8.0\n",
            "Episode:  151  running reward:  175.92975402097375  episode reward:  500.0\n",
            "Episode:  152  running reward:  142.54380321677903  episode reward:  9.0\n",
            "Episode:  153  running reward:  129.23504257342321  episode reward:  76.0\n",
            "Episode:  154  running reward:  108.98803405873858  episode reward:  28.0\n",
            "Episode:  155  running reward:  99.79042724699087  episode reward:  63.0\n",
            "Episode:  156  running reward:  109.03234179759271  episode reward:  146.0\n",
            "Episode:  157  running reward:  160.62587343807417  episode reward:  367.0\n",
            "Episode:  158  running reward:  137.10069875045934  episode reward:  43.0\n",
            "Episode:  159  running reward:  165.28055900036748  episode reward:  278.0\n",
            "Episode:  160  running reward:  178.82444720029397  episode reward:  233.0\n",
            "Episode:  161  running reward:  155.25955776023517  episode reward:  61.0\n",
            "Episode:  162  running reward:  154.40764620818814  episode reward:  151.0\n",
            "Episode:  163  running reward:  169.32611696655053  episode reward:  229.0\n",
            "Episode:  164  running reward:  184.86089357324045  episode reward:  247.0\n",
            "Episode:  165  running reward:  165.48871485859235  episode reward:  88.0\n",
            "Episode:  166  running reward:  179.5909718868739  episode reward:  236.0\n",
            "Episode:  167  running reward:  168.87277750949914  episode reward:  126.0\n",
            "Episode:  168  running reward:  184.69822200759933  episode reward:  248.0\n",
            "Episode:  169  running reward:  150.95857760607944  episode reward:  16.0\n",
            "Episode:  170  running reward:  162.76686208486356  episode reward:  210.0\n",
            "Episode:  171  running reward:  230.21348966789085  episode reward:  500.0\n",
            "Episode:  172  running reward:  211.7707917343127  episode reward:  138.0\n",
            "Episode:  173  running reward:  243.81663338745017  episode reward:  372.0\n",
            "Episode:  174  running reward:  294.45330670996015  episode reward:  497.0\n",
            "Episode:  175  running reward:  286.3626453679681  episode reward:  254.0\n",
            "Episode:  176  running reward:  259.4901162943745  episode reward:  152.0\n",
            "Episode:  177  running reward:  307.5920930354996  episode reward:  500.0\n",
            "Episode:  178  running reward:  315.2736744283997  episode reward:  346.0\n",
            "Episode:  179  running reward:  295.4189395427198  episode reward:  216.0\n",
            "Episode:  180  running reward:  265.93515163417584  episode reward:  148.0\n",
            "Episode:  181  running reward:  239.34812130734068  episode reward:  133.0\n",
            "Episode:  182  running reward:  265.6784970458726  episode reward:  371.0\n",
            "Episode:  183  running reward:  287.3427976366981  episode reward:  374.0\n",
            "Episode:  184  running reward:  280.8742381093585  episode reward:  255.0\n",
            "Episode:  185  running reward:  311.8993904874868  episode reward:  436.0\n",
            "Episode:  186  running reward:  266.91951238998945  episode reward:  87.0\n",
            "Episode:  187  running reward:  275.3356099119916  episode reward:  309.0\n",
            "Episode:  188  running reward:  258.0684879295933  episode reward:  189.0\n",
            "Episode:  189  running reward:  249.05479034367463  episode reward:  213.0\n",
            "Episode:  190  running reward:  232.04383227493972  episode reward:  164.0\n",
            "Episode:  191  running reward:  232.23506581995179  episode reward:  233.0\n",
            "Episode:  192  running reward:  225.38805265596145  episode reward:  198.0\n",
            "Episode:  193  running reward:  280.31044212476917  episode reward:  500.0\n",
            "Episode:  194  running reward:  275.64835369981535  episode reward:  257.0\n",
            "Episode:  195  running reward:  258.9186829598523  episode reward:  192.0\n",
            "Episode:  196  running reward:  210.33494636788183  episode reward:  16.0\n",
            "Episode:  197  running reward:  232.26795709430547  episode reward:  320.0\n",
            "Episode:  198  running reward:  213.01436567544437  episode reward:  136.0\n",
            "Episode:  199  running reward:  256.6114925403555  episode reward:  431.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}